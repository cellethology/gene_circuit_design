defaults:
  - predictor: ridge_regressor
  - query_strategy: topk
  - initial_selection_strategy: random
  - transforms@feature_transforms: standardize
  - transforms@target_transforms: log_standardize
  - _self_
  - override hydra/launcher: submitit_slurm

# data paths
dataset_name: ${env:AL_DATASET_NAME}
metadata_path: ${env:AL_METADATA_PATH}
embedding_dir: ${env:AL_EMBEDDING_ROOT}
embedding_model: enformer_pca32
embedding_path: ${embedding_dir}/${embedding_model}.npz

# number of random seeds to run for each active learning experiment
num_seeds_per_job: 100
parallelize_seeds: true

# active learning settings
al_settings:
  batch_size: 12
  starting_batch_size: 12
  max_rounds: 10
  feature_transforms: ${feature_transforms}
  target_transforms: ${target_transforms}
  output_dir: ${hydra:runtime.output_dir}
  query_strategy: ${query_strategy}
  label_key: "Fold Change (Induced/Basal)"
  predictor: ${predictor}
  initial_selection_strategy: ${initial_selection_strategy}
  seed: 0

# hydra settings
hydra:
  sweep:
    subdir: ${dataset_name}/${hydra.job.override_dirname}
  sweeper: # multirun mode sweeps over these parameters
    params:
      initial_selection_strategy: random, kmeans
      query_strategy: topk
      predictor: rf_regressor, mlp_regressor, gaussian_regressor
      embedding_model: enformer_pca32, evo2_meanpool_block28_pca8, evo_1p5_pca32
  launcher:
    timeout_min: 720
    # everything below is used only for submitting jobs to the cluster
    partition: wzt_20250411,intel-sc3
    cpus_per_task: 32
    mem_per_cpu: 4GB
    qos: huge
    submitit_folder: slurm_logs
