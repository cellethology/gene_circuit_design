defaults:
  - predictor: botorch_gp
  - query_strategy: botorch_log_ei
  - initial_selection_strategy: core_set
  - transforms@feature_transforms: standardize
  - transforms@target_transforms: log_standardize
  - _self_
  - override hydra/launcher: submitit_slurm

# data paths
datasets_file: datasets/datasets.yaml
dataset_name: ${env:AL_DATASET_NAME}
metadata_path: ${env:AL_METADATA_PATH}
embedding_dir: ${env:AL_EMBEDDING_ROOT}
embedding_model: enformer_evo2_concat
embedding_path: ${embedding_dir}/${embedding_model}.npz
subset_ids_path: ${env:AL_SUBSET_IDS_PATH,null}

# number of random seeds to run for each active learning experiment
num_seeds_per_job: 32
parallelize_seeds: true

# active learning settings
al_settings:
  batch_size: 12
  starting_batch_size: 12
  max_rounds: 9 # not including the initial selection
  feature_transforms: ${feature_transforms}
  target_transforms: ${target_transforms}
  output_dir: ${hydra:runtime.output_dir}
  query_strategy: ${query_strategy}
  label_key: "Fold Change (Induced/Basal)"
  predictor: ${predictor}
  initial_selection_strategy: ${initial_selection_strategy}
  seed: 0

# hydra settings
hydra:
  sweep:
    subdir: ${dataset_name}/${hydra.job.override_dirname}
  sweeper: # multirun mode sweeps over these parameters
    params:
      initial_selection_strategy: core_set
      query_strategy: topk
      predictor: gaussian_regressor
      embedding_model: enformer_evo2_concat
  launcher:
    timeout_min: 160
    # everything below is used only for submitting jobs to the cluster
    partition: intel-sc3,wzt_20250411
    cpus_per_task: 8
    mem_per_cpu: 4GB
    submitit_folder: slurm_logs
